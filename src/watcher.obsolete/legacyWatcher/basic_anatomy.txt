
$Id: basic_anatomy.txt,v 1.5 2007/04/23 18:51:07 dkindred Exp $

The idsCommunications code is organized into several modules, which are of
several types.  The most common type of module is a "simulator land" module.
Simulator land modules are written to the simulator land API, the data module
is an example of one.  The next most common type of module is a clustering
algorithm.  They are also written to the simulator land API, but while there
can be many simulator land modules, there can be only one clustering algorithm.
They implement nodeInit(), nodeFree(), and declare clusteringState, and 
clusteringNeighbor.  Then there are two implementations of the simulator
land API, the simulator itself (in main.cpp), and livenetwork (in
livenetwork.cpp).  The simulator is a simple discrete event simulator,
and livenetwork uses UDP packets.  There is a legendary third implementation
at Telcordia which transports the packets via Opnet.

So to build a binary, one needs an implementation of simulator land, and a
clustering algorithm.  Link them together and you get the clustering algorithm
running either simulated or for real.  The traditional naming scheme is that
the simulator version is just named after the clustering algorithm, and the
livenetwork version has the word live prepended.  (thus the name liveinterim2)

Now, where does the idsCommunications API come into this?  Its just a plain
simulator land module, it just happens to know how to listen on a TCP socket
for connections from clients.  The most interesting result of that is that
you can run it in the simulator, and have real clients connect to it.  That
has not been tried beyond trivial testing however, and the simulator clock
runs too fast (it would need to be tied to wall clock time to properly interact
with clients outside the simulator.).

In simulator land, the idsCommunications API is named packetapi, and is found
in packetapi.(cpp|h).  Read packetcounters.txt [[where is this? -dkindred]]
for an explanation of how it actually sends and receives hierarchy messages, 
but finish this document first.

Back to what simulator land implements:

The lowest level is in des.cpp (Discrete Event Simulator) and livenetwork.cpp.
They implement packet IO (packetSend(), and registering callbacks per packet 
type), timer call-backs, and a system of modules.

A node address is a manetAddr typedef.  Its actually an unsigned int, containing
a host byte order IPv4 address [[in both simulator and livenetwork? -dk]].  
This rather silly decision came from when there was only a simulator, 
and it was actually an index into an array of manetNode structures.  
(That array still exists, manet->nodelist[].)

A packet has a src and dst address, a TTL (set on xmit, and never changed) and
hopcount (incremented on each forward), a packet type (8 bit int) and finally  a
length and payload.  Then for memory allocation purposes, it has a next pointer
and a reference count.  They are left over from when it was a pure simulator,
and delivering the /same/ memory structure to many nodes.

The packet type is defined with a collection of macros in des.h.  Each module
gets a PACKET_MODULENAME macro (in des.h), which is then used in the module's .h
file to define its various packet types with macros named PACKET_MODULENAME_NAME.
(See data.h)

The lowest level packet IO (packetSend()) just throws a packet into the 
ether, there is no reliability, and the only way to determine what to do
with the packet is to look at its addresses and packet type.

Sitting on top of that layer is the data module.  It has a function (callable
by other modules) named dataSend().  Its signature is almost identical to
packetSend(), except that it includes some reliability and routing arguments.

So, a clustering module can create a packet (say of type PACKET_INTERIM2_HELLO),
and send it by calling dataSend() instead of packetSend().  The data
module will take that packet, encapsulate it in a PACKET_DATA_DATA packet,
and toss it into the ether by called packetSend().  It will do the reliability
thing using retransmits (on a timer callback on the sender) and ack packets
(from the data module on the receiver).  On the receiver, the data module
will decapsulate the PACKET_INTERIM2_HELLO packet from the PACKET_DATA_DATA
packet, and hand it back to the simulator land API, which will then (looking at
the packet's type) hand it to the clustering module on the receiver.

That is the basic design philosophy of the idsCommunications networking stack:
arbitrary encapsulation, and then tossing decapsulated packets back to the
start of the entire process, to be decapsulated again.

Continuing the example, lets say the clustering module wanted to send its
packet reliably to some other node, but the ambient routing (Ex: OLSR) is
broken.  (it came to that decision by trying to send it with plain dataSend()).
It can call dataSend() with the routing argument set to DATA_ROUTE_FLOOD.
The data module then does the exact same encapsulation step, but instead of
calling packetSend(), it calls floodSend(), on the flood module.  The flood
module then encapsulates that PACKET_DATA_DATA packet into a PACKET_FLOOD_DATA
packet, sets the destination address to NODE_BROADCAST (also known as
255.255.255.255) and calls packetSend().  Every node within one hop then 
accepts that packet and hands it to its local instance of the flood module.
That flood module then checks to see if it has seen that PACKET_FLOOD_DATA
packets, and if not repeats it, and decapsulates the contained PACKET_DATA_DATA.
That contained packet will get dropped on the floor if its dst addr is
not the local node's...  But on one node it will be correct, and that
data module will accept the packet, decapsulate the clustering module's 
packet, and the final result is that the receiving clustering module has no
idea how its packet was actually sent.


The Data Structures:

The top level is the manet structure, declared in des.h.  It is intended
to be self contained.  It is not entirely true, there are a couple of 
static vars which are related to logging (Blame Michael).  Note that the
manet structure contains packetProtection.  Thats a manet-wide state var
for doing user-land encryption.  The struct is not defined outside of
packetProtection.cpp.  That private pointer pattern is repeated several
times.  Also contained in struct manet are a bunch of DES (Discrete Event
Simulator) variables, a config thing, and a list of manetNode structures.

A manetNode structure contains all the generic node information.  So
things like the nodes location in x,y,z (which needs to be converted 
[[meaning the code should be changed to use lat/lon/alt instead of x/y/z?
or that some existing code does this conversion?  -dkindred]] to
GPS lat/lon/alt), its manetAddr, a bunch of clustering algorithm variables,
a whole bunch of module private state pointers, and a neighbor list.

The neighbor list is maintained using a bunch of functions in node.cpp/h.
The clustering algorithm is expected to update it, see nopcluster.cpp
for an example.  It also contains some private state pointers.

Packets live in packet structures...  and are delivered using EventNode
structures.  Even in the livenetwork case, where the DES event list
is used as the transmit/receive queue.  There is some code for 
debugging the memory allocation of packet structures, they are allocated
in pools, and kept on free lists when not in use, so valgrind and
company are foiled.


How it starts up:

This requires some work...  there is too much knowledge of which modules
are going to be running in main.  Also modules can call modules, and may
need to in their init functions, except that there is no guarantee that
a module has been inited before.

Have a look in main.cpp...
First, call manetInit().  That allocates the node structures, and inits
some pointers.
Then, call manetRFModelInit().  This is separate because its not run
when doing livenetwork.
Call mobilityInit() on each of the node structures.  Also separate from
manetInit() because livenetwork doesn't use it.
Then call firstStep() (des.cpp).  That actually inits the modules.

Then there is some magic with the metrics.  Simulator metrics are 
implemented using a couple of normal DES timer callbacks.  So to 
compute the metrics, just call whatever top level metric function
you want, and it will reschedule itself and just run.  See nodeMetrics()
in node.cpp.

Then, call takeStep() a great many times.  Each call will execute
one timestep of the DES.

Or, to startup in livenetwork mode...
Call manetInit().  
Optionally call packetProtectionInit() (crypto initialization).
Stomp a couple of variables, like the number of nodes, to 1, and
set the single manetNode structure's ManetAddr values.  
Then all firstStep().
Then call takeStep() a great many times... [[not in livenetwork, right? -dk]]


To send a packet:

This is weird...  the weirdness comes from how packet copies were
avoided in the original simulator.

To send a packet you call:
packetMalloc() or packetCopy() to get a packet structure.
Fill out the structure, specifically the p->dst, p->len, and
p->payload fields.
packetSend(), or some other module Send.
packetFree()  

Packets are reference-counted.  packetSend() made a reference
to your packet, and that packetFree() call is really just
decrementing the reference code.  Packet references can be
created with packetDup(), but you may not ever write to 
that packet structure.  Thus when the flood module is 
re-xmitting a packet it just received, it is calling packetCopy(),
then rewriting the header (increment hopcount...) and then
calling packetSend() [[and then packetFree() on its copy? -dk]].


Anatomy of idsCommunications:

The lowest level is in des.cpp and livenetwork.cpp.  They implement packet
IO, timer call-backs, and a system of modules.  The des code is a discrete
event simulator, and can run clustering algorithms.  The livenetwork module
implements the same API but instead actually transmits packets on a network,
and can run the idsCommunications client API as well.

Only one is used at once, selected at link time.  So for each clustering
algorithm, there is (for example) an interim2 binary which is the simulator,
and the liveinterim2 binary which is the live network.

The modules within that framework (which is sometimes referred to as "simulator
land" then implement most of the protocols.

Interesting modules include:

data.cpp    - Reliable delivery of packets, including to multiple recipients.
	      [[Primarily for API client messages.  Is rarely used
	      by clustering algorithms (e.g., PACKET_INTERIM2_ROOT).
	      [["See data.txt"? -dk]]
flood.cpp   - Flood routing of packets, to either the broadcast address, or
              a unicast address.
	      [[used for what?  API client message failover, anything else? -dk]]
	      [[right. -toj]]
	      [[no documentation yet]]
hello.cpp   - Generic hello packets, basically the first iteration of an 
              algorithm somewhere between interim1 and interim2.
              [[is this used by some/all clustering algorithms? -dk]]
	      [[testhello clustering algorithm is used to test this module.]]
	      [[documented somewhere? -dk]]
routing.cpp - A routing algorithm not entirely unlike AODV.  It is reactive,
              but not source routed.  Can be used by the data module for explicitly
              routing unicast packets (e.g., in the simulator).
              [[documented somewhere? -dk]]
packetAPI   - The idsCommunications client API (packetapi.cpp).
              [["See packetapi.txt"? -dk]]

The clustering algorithms are also modules:

amroute.cpp    - AMRoute algorithm.  Only runs in simulator.  Telcordia-owned.
bft.cpp        - Breadth First Tree, does a spanning tree of the MANET, and
                 calls it a hierarchy.  (Experimental simple clustering algorithm.
                 Will only run in simulator.)  Mostly of historical interest.
		 Believed to be generally inferior to interim2; no intelligent
		 positioning of root node.
gmcluster.cpp  - Group mobility based clustering -- uses hello module, leverages
	         GPS data to assign nodes to groups, exports group IDs to 
		 interim2.  Not a clustering module per se; always used in 
		 conjunction w/ interim2.
interim.cpp    - The interim algorithm.
	         [[Documented?  Of mainly historical interest? -dk]]
interim2.cpp   - The (TTA) interim2 algorithm [["See interim2.txt"? -dk]]
nopcluster.cpp - the NOP "algorithm", it doesn't actually create a hierarchy, but
                 instead is used for profiling, and documenting the clustering
                 algorithm API.
justneighbors.cpp - Neighbor sensing only, does not form a hierarchy, but
                    accepts commands (via StateVector api - see
		    idsCommunications.h, and statetest.c) from clients
		    to establish hierarchy relationships

The packetAPI module is the strangest module in the system, in that it breaks
the simulatorland encapsulation, to implement the idsCommunications API.  It
thus allows external programs to communicate with simulator land, and use the
packet IO code as well as all the other modules.

There is also some library code which is not in a formal module.  That includes
the mobility model, metrics code, IDMEF message parsing (using libxml and
libidmef), and message compression (using zlib), as well as several scripts.
The scripts include one to generate tealab files from simulation log files,
running experiments on Sparta's testbed, and plotting various statistics.

The idsCommunications API (see idsCommunications.h) then offers to its clients:
- information on the geometry of the MANET, by snooping the neighbor lists
  maintained by the clustering algorithms
- reliable delivery of messages, using the data module
- routing of messages via ambient routing, or the flood and routing modules
- logging and debugging using the communicationsLog calls (IE: goodwin)

Then sitting on top of the idsCommunications API is:
demo detector - really four example clients, the demodetector, demoaggregator,
              - demorewriter, and demoresponder.
watcher       - GUI display of the MANET, which assumes a control network.
	        (see watcher.txt)
pgraphwatcher - translates watcher telemetry into a format which the pgraph 
                tool can display
glancer       - non-GUI display of a single node's neighbors.
goodwin       - non-GUI logger of watcher telemetry, which can record log files
                to be displayed later using watcher.
routingdetector - Inspects a node's routing table, and generates watcher
                  telemetry.
labeltest     - command line tool to display a label in the watcher display
                (by inserting a message into the telemetry)
edgetest      - command line tool to display an edge in the watcher display
                (by inserting a message into the telemetry)
testapi       - test bracket which can send and receive arbitrary messages

The watcher is interesting in that most of its telemetry is delivered using the
hierarchy itself.
